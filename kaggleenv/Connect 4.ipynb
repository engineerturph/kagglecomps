{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d06f451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment lux_ai_2022 failed: No module named 'vec_noise'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make, evaluate, utils\n",
    "import numpy as np\n",
    "import random\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a00dad",
   "metadata": {},
   "source": [
    "<img src=\"./boardexample.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "obs.board property in this board will be\n",
    "\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b3be8",
   "metadata": {},
   "source": [
    "I don't know where to start than i will copy another code with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e5514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConnectFourGym:\n",
    "\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make('connectx', debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        #We define actions possible here\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        #We define all of space we are playing\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=2,\n",
    "                                            shape=(self.rows, self.columns, 1),\n",
    "                                            dtype=float)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return self.obs['board']\n",
    "\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1:\n",
    "            return 1\n",
    "        elif done:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid:\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else:\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return self.obs['board'],reward,done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b31e2",
   "metadata": {},
   "source": [
    "I am getting state action reward values with any agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679243b",
   "metadata": {},
   "source": [
    "Next i will use q learning \n",
    "- I will use algorithm from this youtube video\n",
    "- https://www.youtube.com/watch?v=wc-FxNENg9U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299bfa1",
   "metadata": {},
   "source": [
    "<img src=\"./loss.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7caa5a",
   "metadata": {},
   "source": [
    "Lets define constants for changing if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176a10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr,input_dims, fc1_dims, fc2_dims,n_actions):\n",
    "        super(DeepQNetwork,self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims,self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cpu')\n",
    "        self.to(self.device)\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size,\n",
    "                     n_actions, max_mem_size=100000, eps_end=0.01, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "            \n",
    "        self.Q_eval = DeepQNetwork(self.lr, n_actions = n_actions,\n",
    "                                       input_dims=input_dims, fc1_dims = 256, fc2_dims= 256)\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "            \n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def choose_action(self,observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item() ##converts tensor action to int\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size: ##Printlemiyorsa bunun yuzunden\n",
    "            return\n",
    "        self.mem_cntr=1000\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False) \n",
    "        ##chooses random batch_size numbers from 0-patch_size range\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch] ##We dont give actions as input we get all of actions as input\n",
    "        \n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch] \n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma * T.max(q_next,dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target,q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon -self.eps_dec if self.epsilon > self.eps_min \\\n",
    "                        else self.eps_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373c45f",
   "metadata": {},
   "source": [
    "Copy pasted this cell down there to get learning curve plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "932f5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KaggleEnv = ConnectFourGym()\n",
    "n_actions = KaggleEnv.action_space.n\n",
    "input_x = KaggleEnv.observation_space.shape[0]*KaggleEnv.observation_space.shape[1]\n",
    "batch_size_const = 128\n",
    "n_games_const = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe2e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -1.00 average score -1.00 epsilon 1.00\n",
      "episode  1 score 1.00 average score 0.00 epsilon 1.00\n",
      "episode  2 score -1.00 average score -0.33 epsilon 1.00\n",
      "episode  3 score -10.00 average score -2.75 epsilon 1.00\n",
      "episode  4 score -10.00 average score -4.20 epsilon 1.00\n",
      "episode  5 score -1.00 average score -3.67 epsilon 1.00\n",
      "episode  6 score -1.00 average score -3.29 epsilon 1.00\n",
      "episode  7 score -1.00 average score -3.00 epsilon 1.00\n",
      "episode  8 score -1.00 average score -2.78 epsilon 1.00\n",
      "episode  9 score 1.00 average score -2.40 epsilon 1.00\n",
      "episode  10 score 1.00 average score -2.09 epsilon 1.00\n",
      "episode  11 score -1.00 average score -2.00 epsilon 1.00\n",
      "episode  12 score 1.00 average score -1.77 epsilon 1.00\n",
      "episode  13 score -1.00 average score -1.71 epsilon 0.99\n",
      "episode  14 score -1.00 average score -1.67 epsilon 0.99\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 14\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     observation_,reward,done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     16\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mreward\n",
      "Cell \u001b[0;32mIn [13], line 55\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[1;32m     54\u001b[0m     state \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mtensor([observation])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_eval\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 55\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     action \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39margmax(actions)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m##converts tensor action to int\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [13], line 16\u001b[0m, in \u001b[0;36mDeepQNetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     18\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Long"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = KaggleEnv\n",
    "    agent = Agent(gamma=0.99, epsilon=1.0, batch_size=batch_size_const, \n",
    "                  n_actions=n_actions,eps_end=0.01,input_dims=[input_x],\n",
    "                  lr=0.0001)\n",
    "    scores, eps_history = [], []\n",
    "    n_games = n_games_const\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_,reward,done = env.step(action)\n",
    "            score +=reward\n",
    "            agent.store_transition(observation,action,reward,observation_,done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        \n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        \n",
    "        print('episode ',i, 'score %.2f' %score,\n",
    "             'average score %.2f' %avg_score,\n",
    "             'epsilon %.2f' %agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40886c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761adf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
