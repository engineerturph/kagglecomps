{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1b52f4",
   "metadata": {},
   "source": [
    "I am gonne use Deep Q learning for this project.CNN would be a lot better and algorithms with N step lookahead algorithms seems to work better than that i will maybe try them after i learn more about monte carlo tree search.\n",
    "\n",
    "Most of good algorithms combine policy learning(really similar to Q learning) and tree search algorithms.\n",
    "\n",
    "https://jonathan-hui.medium.com/alphago-how-it-works-technically-26ddcc085319\n",
    "\n",
    "neural network code i will use later =codelateruse.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d06f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make, evaluate, utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gym import spaces\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a00dad",
   "metadata": {},
   "source": [
    "<img src=\"./boardexample.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "obs.board property in this board will be\n",
    "\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b3be8",
   "metadata": {},
   "source": [
    "I don't know where to start than i will copy another code with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e5514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConnectFourGym:\n",
    "\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make('connectx', debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        #We define actions possible here\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        #We define all of space we are playing\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=2,\n",
    "                                            shape=(self.rows, self.columns, 1),\n",
    "                                            dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return self.obs['board']\n",
    "\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1:\n",
    "            return 1\n",
    "        elif done:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def step(self, agent):\n",
    "        action = agent(self.obs)\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid:\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else:\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return [self.obs['board'],reward,done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29f2730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ConnectFourGym()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b31e2",
   "metadata": {},
   "source": [
    "I am getting state action reward values with any agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29cc1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(agent,repeat_num=1000):\n",
    "    i=0\n",
    "    innerarr = []\n",
    "    outerarr = []\n",
    "    innerarr.append(env.reset())\n",
    "    while i<repeat_num:\n",
    "        new_state = env.step(agent)\n",
    "        innerarr.append(new_state)\n",
    "        if new_state[2]==True:\n",
    "            outerarr.append(innerarr)\n",
    "            innerarr = []\n",
    "            i += 1\n",
    "            innerarr.append(env.reset())\n",
    "    return outerarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679243b",
   "metadata": {},
   "source": [
    "Next i will use q learning \n",
    "- At first agent will move 100% random and it will decrease after every 100 game and agent will move according to neural network 1%(q_increase) more.It will stop at 97% (q_highest_prob).\n",
    "- After every 1000 game we will update Q (repeat_num)\n",
    "- Targets w will update after every 5(target_update_interval) updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299bfa1",
   "metadata": {},
   "source": [
    "<img src=\"./loss.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7caa5a",
   "metadata": {},
   "source": [
    "Lets define constants for changing if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bfac41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_increase = 0.01\n",
    "repeat_num = 1000\n",
    "target_update_interval = 5\n",
    "q_highest_prob = 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7650bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "model = Sequential(\n",
    "    [Dense(64, activation='relu'),\n",
    "     Dense(32, activation='relu'),\n",
    "     Dense(7)])\n",
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "176a10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_loss(reward):\n",
    "  def custom_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true) - K.square(y_true - reward), axis=-1)\n",
    "\n",
    "  return custom_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f92894",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = custom_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
