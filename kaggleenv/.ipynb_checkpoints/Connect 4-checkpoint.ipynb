{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1b52f4",
   "metadata": {},
   "source": [
    "I am gonne use Deep Q learning for this project.CNN would be a lot better and algorithms with N step lookahead algorithms seems to work better than that i will maybe try them after i learn more about monte carlo tree search.\n",
    "\n",
    "Most of good algorithms combine policy learning(really similar to Q learning) and tree search algorithms.\n",
    "\n",
    "https://jonathan-hui.medium.com/alphago-how-it-works-technically-26ddcc085319\n",
    "\n",
    "neural network code i will use later =codelateruse.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d06f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make, evaluate, utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gym import spaces\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a00dad",
   "metadata": {},
   "source": [
    "<img src=\"./boardexample.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "obs.board property in this board will be\n",
    "\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b3be8",
   "metadata": {},
   "source": [
    "I don't know where to start than i will copy another code with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1e5514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConnectFourGym:\n",
    "\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make('connectx', debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        #We define actions possible here\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        #We define all of space we are playing\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=2,\n",
    "                                            shape=(self.rows, self.columns, 1),\n",
    "                                            dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return self.obs['board']\n",
    "\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1:\n",
    "            return 1\n",
    "        elif done:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def step(self, agent):\n",
    "        action = agent(self.obs)\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid:\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else:\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return self.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e29f2730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ConnectFourGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "580b8ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8074f",
   "metadata": {},
   "source": [
    "I will use similar neural network that i used before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d76f5955",
   "metadata": {
    "toggle": true
   },
   "outputs": [],
   "source": [
    "def dq_agent(state):\n",
    "    valid_moves = [col for col in range(7) if state['board'][col] == 0]\n",
    "    return random.choice(valid_moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b31e2",
   "metadata": {},
   "source": [
    "I will try to get state action reward values with any agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29cc1a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1], 'mark': 1}\n",
      "{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 1], 'mark': 1}\n",
      "{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 1], 'mark': 1}\n",
      "{'remainingOverageTime': 60, 'step': 8, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 2, 0, 0, 2, 1], 'mark': 1}\n",
      "{'remainingOverageTime': 60, 'step': 10, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 1, 2, 0, 0, 2, 1], 'mark': 1}\n",
      "{'remainingOverageTime': 60, 'step': 12, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 1, 2, 0, 1, 2, 1], 'mark': 1}\n",
      "{'remainingOverageTime': 60, 'step': 14, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 2, 1, 1, 2, 0, 1, 2, 1], 'mark': 1}\n",
      "{'remainingOverageTime': 60, 'step': 16, 'board': [0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 2, 1, 1, 2, 0, 1, 2, 1], 'mark': 1}\n"
     ]
    },
    {
     "ename": "FailedPrecondition",
     "evalue": "Environment done, reset required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdq_agent\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn [51], line 37\u001b[0m, in \u001b[0;36mConnectFourGym.step\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     35\u001b[0m is_valid \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboard\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mint\u001b[39m(action)] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs, old_reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchange_reward(old_reward, done)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle_environments/core.py:422\u001b[0m, in \u001b[0;36mEnvironment.train.<locals>.step\u001b[0;34m(action)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(action):\n\u001b[1;32m    421\u001b[0m     actions, logs \u001b[38;5;241m=\u001b[39m runner\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m     advance()\n\u001b[1;32m    424\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_shared_state(position)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle_environments/core.py:209\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, actions, logs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     logs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FailedPrecondition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment done, reset required.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m actions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(actions) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidArgument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m actions required.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: Environment done, reset required."
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(10):\n",
    "    print(env.step(dq_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2cbed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
