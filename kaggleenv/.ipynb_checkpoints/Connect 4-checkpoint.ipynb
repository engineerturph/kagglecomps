{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d06f451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment lux_ai_2022 failed: No module named 'vec_noise'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make, evaluate, utils\n",
    "import numpy as np\n",
    "import random\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a00dad",
   "metadata": {},
   "source": [
    "<img src=\"./boardexample.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "obs.board property in this board will be\n",
    "\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b3be8",
   "metadata": {},
   "source": [
    "I don't know where to start than i will copy another code with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e5514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConnectFourGym:\n",
    "\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make('connectx', debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        #We define actions possible here\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        #We define all of space we are playing\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=2,\n",
    "                                            shape=(self.rows, self.columns, 1),\n",
    "                                            dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return self.obs['board']\n",
    "\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1:\n",
    "            return 1\n",
    "        elif done:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def step(self, agent):\n",
    "        action = agent(self.obs)\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid:\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else:\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return [self.obs['board'],reward,done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29f2730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ConnectFourGym()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b31e2",
   "metadata": {},
   "source": [
    "I am getting state action reward values with any agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679243b",
   "metadata": {},
   "source": [
    "Next i will use q learning \n",
    "- I will use algorithm from this youtube video\n",
    "- https://www.youtube.com/watch?v=wc-FxNENg9U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299bfa1",
   "metadata": {},
   "source": [
    "<img src=\"./loss.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7caa5a",
   "metadata": {},
   "source": [
    "Lets define constants for changing if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "176a10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr,input_dims, fc1_dims, fc2_dims,n_actions):\n",
    "        super(DeepQNetwork,self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims,self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cpu')\n",
    "        self.to(self.device)\n",
    "    def forwward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size,\n",
    "                     n_actions, max_mem_size=100000, eps_end=0.01, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "            \n",
    "        self.Q_eval = DeepQNetwork(self.lr, n_actions = n_actions,\n",
    "                                       input_dims=input_dims, fc1_dims = 256, fc2_dims= 256)\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "    def store_trainsition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "            \n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def choose_action(self,observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item() ##converts tensor action to int\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        self.mem_cntr=1000\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False) \n",
    "        ##chooses random batch_size numbers from 0-patch_size range\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eddcd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(gamma=0.99,epsilon=1,lr=0.001,input_dims=[10,10],batch_size=100,n_actions=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6816d214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedeb1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7c350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
